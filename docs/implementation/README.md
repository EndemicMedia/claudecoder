# âš™ï¸ Implementation Guide

Technical implementation details, architecture, and tokenization system for ClaudeCoder.

## ğŸ—ï¸ System Architecture

### Core Components
```
ğŸ“ ClaudeCoder Architecture
â”œâ”€â”€ ğŸ¤– AI Providers
â”‚   â”œâ”€â”€ AWS Bedrock (Enterprise)
â”‚   â””â”€â”€ OpenRouter (Free/Premium)
â”œâ”€â”€ ğŸ§  Core Processing
â”‚   â”œâ”€â”€ Core Processor (GitHub Actions)
â”‚   â”œâ”€â”€ Local Processor (CLI)
â”‚   â””â”€â”€ Fallback Manager (Multi-model)
â”œâ”€â”€ ğŸ”§ Tokenization System
â”‚   â”œâ”€â”€ Token Estimator
â”‚   â”œâ”€â”€ Repository Scanner
â”‚   â””â”€â”€ AI Prioritization
â””â”€â”€ ğŸ¯ Output Handlers
    â”œâ”€â”€ GitHub PR Updates
    â””â”€â”€ Local File Changes
```

## ğŸ¯ Tokenization System (Smart Repository Compression)

### Problem Solved
Large repositories exceed AI model context limits, causing failures. ClaudeCoder now automatically compresses repositories by 95%+ while preserving essential functionality.

**Real-world example**: EasyBin repository
- **Before**: 79 files, 1,159,454 tokens (35x over Kimi K2 32K limit)
- **After**: 23 files, 26,129 tokens (80% of limit used)
- **Result**: 97.7% compression with successful AI processing

### Multi-Phase Processing Pipeline
```javascript
ğŸ” Phase 1: Repository Analysis
â”œâ”€â”€ Scan all files and calculate token counts
â”œâ”€â”€ Apply file type detection and prioritization
â”œâ”€â”€ Calculate total tokens vs model limits
â””â”€â”€ Determine if tokenization needed

ğŸ¤– Phase 2: AI-Powered Prioritization (when needed)
â”œâ”€â”€ Send file list and user prompt to AI
â”œâ”€â”€ Get intelligent file selection based on relevance
â”œâ”€â”€ Parse response into critical/important/skip categories
â””â”€â”€ Fall back to heuristic if AI unavailable

âš¡ Phase 3: Content Optimization
â”œâ”€â”€ Apply prioritization results
â”œâ”€â”€ Summarize large but important files
â”œâ”€â”€ Enforce strict token budget limits
â””â”€â”€ Generate final optimized repository
```

### Multi-Model Support
Different AI models have different context windows. ClaudeCoder automatically adjusts:

| Model | Context Limit | Strategy |
|-------|---------------|----------|
| Kimi K2 | 32K tokens | Aggressive compression (keep ~15 files) |
| GPT-4 | 8K tokens | Ultra-aggressive (keep ~8 files) |
| Claude Sonnet | 200K tokens | Moderate compression (keep ~100 files) |
| Gemini 2.0 | 1M tokens | Light compression (keep ~400 files) |

## ğŸ§ª Real-World Validation Results

### EasyBin Repository Case Study
```
ğŸ“Š Input Analysis:
   Files: 79 files total
   Content: 1,159,454 tokens
   Model: Kimi K2 (32,768 token limit)
   Overflow: 3,437% over limit

ğŸ¤– AI Prioritization:
   Critical: README.md, package.json, index.html, app.js
   Important: styles.css, sw.js, manifest.json
   Skip: coverage/, test-results/, playwright-report/

âœ… Final Result:
   Selected: 23 files
   Tokens: 26,129 (80% of limit)
   Compression: 97.7% reduction
   Status: Successfully processed
```

### Performance Benchmarks
- **Small repos** (10 files): <1 second
- **Medium repos** (100 files): <5 seconds  
- **Large repos** (500 files): <15 seconds
- **Memory usage**: <50MB for 100 files

**The tokenization system represents a significant advancement in AI-powered repository processing, enabling ClaudeCoder to handle enterprise-scale repositories while maintaining high-quality code generation capabilities.** ğŸ¯